# -*- coding: utf-8 -*-
"""Copy of face_matching_ekyc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dfZRQf8qVBxeRAwvTHIiDbqAyfQIeo1S
"""

# !pip install tensorflow_gpu==1.15.0

# !pip install git+https://github.com/rcmalli/keras-vggface.git

# !pip install mtcnn

# pip install keras==2.3.1

# !pip uninstall h5py
# !pip install h5py==2.10.0

import keras_vggface
import matplotlib.pyplot as plt
import mtcnn
import numpy as np
from PIL import Image
from mtcnn.mtcnn import MTCNN
import cv2
import math
from numpy import asarray
from keras_vggface.vggface import VGGFace
from keras_vggface.utils import preprocess_input
from keras_vggface.utils import decode_predictions
from scipy.spatial.distance import cosine


#Open web_cam, take images and close camera
def take_img_from_cam():
    cam = cv2.VideoCapture(0)

    cv2.namedWindow("test")

    img_counter = 0

    while True:
        ret, frame = cam.read()
        if not ret:
            print("failed to grab frame")
            break
        cv2.imshow("test", frame)
        k = cv2.waitKey(1)
        if k % 256 == 27:
        # ESC pressed
          print("Escape hit, closing...")
          break
        elif k % 256 == 32:
        # SPACE pressed
          img_name = "opencv_frame_{}.jpg".format(img_counter)
          cv2.imwrite(img_name, frame)
          print("{} written!".format(img_name))
          img_counter += 1
    cam.release()
    cv2.destroyAllWindows()
    return


# extract a single face from a given photograph
def extract_face(filename, required_size=(224, 224)):
    # load image from file
    pixels = plt.imread(filename)

    # create the detector, using default weights
    detector = MTCNN()
    # detect faces in the image
    results = detector.detect_faces(pixels)
    #Extract keypoints from the first face
    keypoints = results[0]['keypoints']
    # extract the bounding box from the first face
    x1, y1, width, height = results[0]['box']
    x2, y2 = x1 + width, y1 + height
    # extract the face
    face = pixels[y1:y2, x1:x2]
    # resize pixels to the model size
    image = Image.fromarray(face)
    image = image.resize(required_size)
    face_array = asarray(image)
    return face_array, keypoints


# load the photo and extract the face
# pixels = extract_face('image_1.jpg')

# plot the extracted face
# plt.imshow(pixels)
# show the plot

# create a vggface model
model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')


# extract faces and calculate face embeddings for a list of photo fil


def check_right(righteye, lefteye, nose, orig_eye_dist, orig_nose_x):
    i=0
    dist = math.sqrt((righteye[0] - lefteye[0]) ** 2 + (righteye[1] - lefteye[1]) ** 2)
    if dist <= orig_eye_dist * 0.52 and nose[0] < orig_nose_x:
        print('right_moved')
        i = i+1
    else:
        print('no right movement')
    return i


def check_left(righteye, lefteye, nose, orig_eye_dist, orig_nose_x):
    i=0
    dist = math.sqrt((righteye[0] - lefteye[0]) ** 2 + (righteye[1] - lefteye[1]) ** 2)

    if dist <= orig_eye_dist * 0.52 and nose[0] > orig_nose_x:
        print('left moved')
        i=i+1
    else:
        print('no left movement')
    return i


def check_smile(mouth_right, mouth_left, orig_mouth_dist):
    i=0
    dist = math.sqrt((mouth_right[0] - mouth_left[0]) ** 2 + (mouth_right[1] - mouth_left[1]) ** 2)
    print(dist)
    if dist >= orig_mouth_dist * 1.1:
        print('smiled')
        i= i+1
    else:
        print('no smile detected')
    return i


def check_pout(mouth_right, mouth_left, orig_mouth_dist):
    i=0
    dist = math.sqrt((mouth_right[0] - mouth_left[0]) ** 2 + (mouth_right[1] - mouth_left[1]) ** 2)
    print(dist)
    if dist <= orig_mouth_dist - 5:
        print('pout detected')
        i=i+1
        return True
    else:
        print('no pout')
        return False
    return i


def original_keypoints(image):
    facearray,kp_normal = extract_face(image, required_size=(224, 224))

    original_eye_dist = math.sqrt((kp_normal['right_eye'][0]-kp_normal['left_eye'][0])**2 + (kp_normal['right_eye'][1]-kp_normal['left_eye'][1])**2)

    print("Original Eye Distance = ",original_eye_dist)

    original_mouth_dist = math.sqrt((kp_normal['mouth_right'][0]-kp_normal['mouth_left'][0])**2 + (kp_normal['mouth_right'][1]-kp_normal['mouth_left'][1])**2)

    print("Original Lip Distance = ",original_mouth_dist)

    original_nose_x = kp_normal['nose'][0]

    return original_eye_dist,original_mouth_dist,original_nose_x


def liveliness_check(liveliness_photo, original_eye_dist, original_mouth_dist, original_nose_x,i=0):
    _, keypoints = extract_face(liveliness_photo[1], required_size=(224, 224))
    status_right = check_right(keypoints['right_eye'], keypoints['left_eye'], keypoints['nose'], original_eye_dist,
                               original_nose_x)

    _, keypoints = extract_face(liveliness_photo[2], required_size=(224, 224))
    status_left = check_left(keypoints['right_eye'], keypoints['left_eye'], keypoints['nose'], original_eye_dist,
                         original_nose_x)

    _, keypoints = extract_face(liveliness_photo[3], required_size=(224, 224))
    status_smile = check_smile(keypoints['mouth_right'], keypoints['mouth_left'], original_mouth_dist)

    _, keypoints = extract_face(liveliness_photo[4], required_size=(224, 224))
    status_pout = check_pout(keypoints['mouth_right'], keypoints['mouth_left'], original_mouth_dist)

    return status_right,status_left,status_smile,status_pout


#taking pictures of voters id and selfie
take_img_from_cam()

# define filenames
#filenames = ['opencv_frame_0.jpg', 'opencv_frame_1.jpg',]
liveliness_photo= ['opencv_frame_0.jpg','opencv_frame_1.jpg','opencv_frame_2.jpg','opencv_frame_3.jpg','opencv_frame_4.jpg']



original_eye_dist,original_mouth_dist,original_nose_x = original_keypoints(liveliness_photo[0])
status_right,status_left,status_smile,status_pout = liveliness_check(liveliness_photo, original_eye_dist, original_mouth_dist, original_nose_x)
if (status_right == True) and (status_left == True) and (status_smile == True) and (status_pout == True):
    print("Liveliness Check completed successfully ")
else:
    print("Lveliness Check failed.")




# Capturing real time video stream.
video_capture = cv2.VideoCapture(0)
# get vcap property
width  = video_capture.get(3) # float
height = video_capture.get(4) # float
press_flag = False
cmd = ""
